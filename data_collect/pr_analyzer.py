import ast
import re
import json
import time
import toml
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import openai
from pathlib import Path
import requests
import base64
from utils import is_test_file
from tqdm import tqdm

# --- Configuration Loading ---
def load_config():
    """Load configuration file"""
    config_file = Path(__file__).parent / "config.toml"
    with open(config_file, 'r', encoding='utf-8') as f:
        return toml.load(f)

CONFIG = load_config()

# --- Configuration Area ---
GITHUB_TOKEN = CONFIG['common']['github_token']
OPENAI_API_KEY = CONFIG['common']['openai_api_key']
OPENAI_MODEL = CONFIG['common']['openai_model']

# Cache file
PR_ANALYSIS_CACHE_FILE = Path(__file__).parent / CONFIG['common']['output_dir'] / CONFIG['pr_analyzer']['pr_analysis_cache_file']

# GitHub API base URL
GITHUB_API_BASE = CONFIG['common']['github_api_base']

HEADERS = {
    'Authorization': f'token {GITHUB_TOKEN}',
    'Accept': 'application/vnd.github.v3+json'
}

# --- Data Class Definitions ---

@dataclass
class TestFile:
    """Represents a test file"""
    path: str
    content: str
    size: int
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'TestFile':
        return cls(**data)

@dataclass
class FileChange:
    """Represents file change information"""
    filename: str
    status: str  # 'added', 'removed', 'modified', 'renamed'
    additions: int
    deletions: int
    changes: int
    patch: Optional[str] = None  # diff content
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'FileChange':
        return cls(**data)

@dataclass
class Commit:
    """Represents a Git commit"""
    sha: str
    message: str
    date: str
    author: str
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'Commit':
        return cls(**data)

@dataclass
class PRAnalysis:
    """Represents detailed analysis result of a PR"""
    pr_number: str
    repo_name: str
    title: str
    description: str
    state: str  # 'open', 'closed', 'merged'
    merged: bool
    base_commit: Commit  # commit info before PR
    head_commit: Commit  # commit info after PR
    file_changes: List[FileChange]
    detailed_description: str  # detailed description generated by LLM based on file changes
    has_tests: bool  # whether related tests were found
    test_files: List[str]  # test file path list
    only_modified_existing_functions: bool # whether only existing functions were modified
    non_test_files: List[str] # non-test file path list
    analyzed_at: str
    
    def to_dict(self) -> Dict:
        return {
            'pr_number': self.pr_number,
            'repo_name': self.repo_name,
            'title': self.title,
            'description': self.description,
            'state': self.state,
            'merged': self.merged,
            'base_commit': self.base_commit.to_dict(),
            'head_commit': self.head_commit.to_dict(),
            'file_changes': [fc.to_dict() for fc in self.file_changes],
            'detailed_description': self.detailed_description,
            'has_tests': self.has_tests,
            'test_files': self.test_files,
            'only_modified_existing_functions': self.only_modified_existing_functions,
            'non_test_files': self.non_test_files or [],
            'analyzed_at': self.analyzed_at
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'PRAnalysis':
        return cls(
            pr_number=data['pr_number'],
            repo_name=data['repo_name'],
            title=data['title'],
            description=data['description'],
            state=data['state'],
            merged=data['merged'],
            base_commit=Commit.from_dict(data.get('base_commit', {})),
            head_commit=Commit.from_dict(data.get('head_commit', {})),
            file_changes=[FileChange.from_dict(fc) for fc in data.get('file_changes', [])],
            detailed_description=data.get('detailed_description', ''),
            has_tests=data.get('has_tests', False),
            test_files=data.get('test_files', []),
            only_modified_existing_functions=data.get('only_modified_existing_functions', True),
            non_test_files=data.get('non_test_files', []),
            analyzed_at=data.get('analyzed_at', '')
        )

@dataclass
class EnhancedFeature:
    """Enhanced feature object containing PR detailed analysis"""
    feature_type: str
    description: str
    pr_analyses: List[PRAnalysis]
    feature_detailed_description: str  # overall detailed description based on all PR analyses
    
    def to_dict(self) -> Dict:
        return {
            'feature_type': self.feature_type,
            'description': self.description,
            'pr_analyses': [pr.to_dict() for pr in self.pr_analyses],
            'feature_detailed_description': self.feature_detailed_description
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'EnhancedFeature':
        return cls(
            feature_type=data['feature_type'],
            description=data['description'],
            pr_analyses=[PRAnalysis.from_dict(pr) for pr in data.get('pr_analyses', [])],
            feature_detailed_description=data.get('feature_detailed_description', '')
        )

# --- Cache Management ---

def load_pr_analysis_cache() -> Dict[str, PRAnalysis]:
    """Load PR analysis cache"""
    if PR_ANALYSIS_CACHE_FILE.exists():
        try:
            with open(PR_ANALYSIS_CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                cache = {}
                for key, pr_data in data.items():
                    cache[key] = PRAnalysis.from_dict(pr_data)
                print(f"✅ Loaded {len(cache)} PR analysis results from cache")
                return cache
        except Exception as e:
            print(f"⚠️ Failed to load PR analysis cache: {e}")
            return {}
    return {}

def save_pr_analysis_to_cache(analysis: PRAnalysis):
    """Save PR analysis result to cache"""
    cache = {}
    if PR_ANALYSIS_CACHE_FILE.exists():
        try:
            with open(PR_ANALYSIS_CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
        except:
            pass
    
    cache_key = f"{analysis.repo_name}#{analysis.pr_number}"
    cache[cache_key] = analysis.to_dict()
    
    try:
        with open(PR_ANALYSIS_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)
        print(f"💾 Saved analysis result for PR#{analysis.pr_number} to cache")
    except Exception as e:
        print(f"⚠️ Failed to save PR analysis cache: {e}")

# --- GitHub API Functions ---

def extract_pr_number_from_url(pr_url: str) -> Optional[str]:
    """Extract PR number from PR URL"""
    match = re.search(r'/pull/(\d+)', pr_url)
    return match.group(1) if match else None

def get_pr_info(repo_name: str, pr_number: str) -> Optional[Dict]:
    """Get PR basic information"""
    url = f"{GITHUB_API_BASE}/repos/{repo_name}/pulls/{pr_number}"
    
    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            return response.json()
        else:
            print(f"⚠️ Failed to get PR#{pr_number} info: {response.status_code}")
            return None
    except Exception as e:
        print(f"⚠️ Exception getting PR#{pr_number} info: {e}")
        return None

def get_pr_files(repo_name: str, pr_number: str) -> List[FileChange]:
    """Get PR file change information"""
    url = f"{GITHUB_API_BASE}/repos/{repo_name}/pulls/{pr_number}/files"
    
    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            files_data = response.json()
            file_changes = []
            
            for file_data in files_data:
                file_change = FileChange(
                    filename=file_data.get('filename', ''),
                    status=file_data.get('status', ''),
                    additions=file_data.get('additions', 0),
                    deletions=file_data.get('deletions', 0),
                    changes=file_data.get('changes', 0),
                    patch=file_data.get('patch', '')
                )
                file_changes.append(file_change)
            
            return file_changes
        else:
            print(f"⚠️ Failed to get PR#{pr_number} file changes: {response.status_code}")
            return []
    except Exception as e:
        print(f"⚠️ Exception getting PR#{pr_number} file changes: {e}")
        return []

def get_file_content(repo_name: str, file_path: str, ref: str) -> Optional[str]:
    """Get file content"""
    try:
        url = f"{GITHUB_API_BASE}/repos/{repo_name}/contents/{file_path}"
        time.sleep(0.3)
        response = requests.get(url, headers=HEADERS, params={'ref': ref})
        
        if response.status_code == 200:
            content_data = response.json()
            if content_data.get('encoding') == 'base64':
                content = base64.b64decode(content_data['content']).decode('utf-8', errors='ignore')
                return content
    except Exception as e:
        print(f"    - Failed to get file content {file_path}: {e}")
    
    return None

def get_commit_info(repo_name: str, commit_sha: str) -> Optional[Commit]:
    """Get detailed information of a single commit"""
    url = f"{GITHUB_API_BASE}/repos/{repo_name}/commits/{commit_sha}"
    
    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            commit_data = response.json()
            return Commit(
                sha=commit_data.get('sha', ''),
                message=commit_data.get('commit', {}).get('message', ''),
                date=commit_data.get('commit', {}).get('author', {}).get('date', ''),
                author=commit_data.get('commit', {}).get('author', {}).get('name', '')
            )
        else:
            print(f"⚠️ Failed to get commit {commit_sha[:8]} info: {response.status_code}")
            return None
    except Exception as e:
        print(f"⚠️ Exception getting commit {commit_sha[:8]} info: {e}")
        return None

def extract_definitions(content: str) -> List[str]:
    """Extract function and class definitions from Python code content, including nested relationships"""
    if not content:
        return []
    
    try:
        # Parse code into AST
        tree = ast.parse(content)
        
        # Store all definitions (including their paths)
        definitions = []
        
        def visit_node(node, path=""):
            """Recursively visit nodes and collect definition information"""
            if isinstance(node, ast.FunctionDef):
                full_name = f"{path}.{node.name}" if path else node.name
                definitions.append(full_name)
                
                # Recursively visit definitions within function body
                for child in node.body:
                    visit_node(child, full_name)
                    
            elif isinstance(node, ast.ClassDef):
                full_name = f"{path}.{node.name}" if path else node.name
                definitions.append(full_name)
                
                # Recursively visit definitions within class body
                for child in node.body:
                    visit_node(child, full_name)
            
            elif isinstance(node, ast.Module):
                # Top-level node of module
                for child in node.body:
                    visit_node(child)
        
        # Start visiting
        visit_node(tree)
        return definitions
        
    except SyntaxError:
        print(f"    - ⚠️ Code parsing error, may contain syntax errors")
        return []

def analyze_function_changes(before_content: str, after_content: str) -> Tuple[bool, List[str], List[str]]:
    """Analyze function changes, return whether only existing functions were modified and lists of added/deleted functions"""
    
    # Analyze file content changes (using full paths to distinguish nested relationships)
    before_definitions = set(extract_definitions(before_content or ""))
    after_definitions = set(extract_definitions(after_content or ""))
    
    # Find added and deleted definitions (including functions and classes)
    new_definitions = list(after_definitions - before_definitions)
    deleted_definitions = list(before_definitions - after_definitions)
    
    # Determine if only existing functions and classes were modified
    only_modified = len(new_definitions) == 0 and len(deleted_definitions) == 0
    
    return only_modified, new_definitions, deleted_definitions

# --- LLM Analysis ---

def generate_detailed_description_with_llm(
    feature_description: str,
    pr_info: Dict,
    file_changes: List[FileChange]
) -> Optional[str]:
    """Use LLM to generate detailed feature description based on file changes"""
    
    client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url="")
    
    # Filter out test files, only build summary for non-test files
    non_test_file_changes = [fc for fc in file_changes if not is_test_file(fc.filename)]
    
    # Read parameters from config
    max_files = CONFIG['pr_analyzer']['max_files_in_summary']
    max_patch_length = CONFIG['pr_analyzer']['max_patch_length']
    max_patch_preview = CONFIG['pr_analyzer']['max_patch_preview_length']
    
    # Build file change summary
    files_summary = []
    for fc in non_test_file_changes[:max_files]:  # Use config file count limit
        summary = f"- {fc.filename} ({fc.status}): +{fc.additions}/-{fc.deletions}"
        if fc.patch and len(fc.patch) < max_patch_length:  # Use config patch length limit
            summary += f"\n  {fc.patch[:max_patch_preview]}..."  # Use config preview length
        files_summary.append(summary)
    
    files_text = "\n".join(files_summary)
    if len(non_test_file_changes) > max_files:
        files_text += f"\n... and {len(non_test_file_changes) - max_files} more files"
    
    # If no non-test files, provide prompt info
    if not non_test_file_changes:
        files_text = "No files were modified in this PR."
    
    prompt = f"""
You are creating a user requirement description that will be used to instruct another LLM to implement the exact same functionality. Your task is to analyze the PR information and code changes, then write a comprehensive user request that describes what the user wants to accomplish.

This description will be given to a coding LLM to generate the implementation, so you must ensure all functionality across all modified files is thoroughly described from the user's perspective.

Original Feature Description: {feature_description}

PR Title: {pr_info.get('title', '')}
PR Description: {pr_info.get('body', '')}

File Changes:
{files_text}

Your user requirement description must:
- Start with "I want to" and write as if a user is requesting this functionality from a developer
- Include ALL information from the original PR Description - do not omit any details, requirements, or context provided there
- Describe what the user wants to accomplish with complete detail for every file that was modified
- Include all functional requirements that would be needed to recreate this exact implementation
- When functions/methods have parameter changes (additions, deletions, modifications), describe what the user needs in terms of input data and configuration options, mentioning specific parameter names naturally within the context of the requirement
- Focus on the complete user workflow and all capabilities they need
- Describe the expected behavior and outcomes the user wants to achieve
- Ensure a coding LLM reading this could implement all the functionality without seeing the original code
- Write as a natural user request, not technical documentation
- Avoid phrases like "implement function X" or "modify file Y" - instead describe what the user wants to accomplish
- Do NOT include any actual code implementations, code snippets, or technical syntax - only describe the desired functionality and behavior from a user perspective
- Focus on WHAT the user wants to achieve, not HOW it should be implemented technically

Remember: This description will be the only guide for another LLM to recreate this functionality, so include every important detail about what the user wants to achieve, but express it as natural user requirements. You must incorporate all information from the original PR description. Never include code - only describe the desired outcomes and behaviors.
"""

    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "You are a requirements analyst who creates comprehensive user requirement descriptions that will be used to instruct coding LLMs to implement functionality. Your descriptions must be detailed enough for another LLM to recreate the exact same implementation without seeing the original code. Always start with 'I want to' and write from the user's perspective about what they need to accomplish."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=8192
        )

        content = response.choices[0].message.content
        return content if content is not None else feature_description
        
    except Exception as e:
        print(f"⚠️ LLM failed to generate detailed description: {e}")
        return feature_description

def generate_feature_detailed_description(
    feature_description: str,
    feature_type: str,
    pr_analyses: List[PRAnalysis]
) -> Optional[str]:
    """Generate detailed description for the entire feature based on detailed analyses of multiple PRs"""
    
    client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url="")
    
    # Build detailed summary of all PRs
    pr_summaries = []
    for pr in pr_analyses:
        summary = f"""
PR #{pr.pr_number}: {pr.title}
- Status: {pr.state} (merged: {pr.merged})
- Files changed: {len(pr.file_changes)}
- User benefits: {pr.detailed_description}
"""
        pr_summaries.append(summary)
    
    prs_text = "\n".join(pr_summaries)
    
    prompt = f"""
You need to create a comprehensive user-focused description for this complete feature. Analyze all the related pull requests to understand what this feature enables users to do.

Feature Type: {feature_type}
Original Description: {feature_description}

Associated Pull Requests:
{prs_text}

Your description must:
- Start with "I want to" and focus on what users can accomplish
- Explain the specific capabilities this complete feature provides
- Show how all related changes work together to benefit users
- Highlight what new things users can now do that they couldn't before
- Demonstrate how this improves user workflow and productivity
- Explain the practical value users receive from this feature
- Combine insights from all PRs to show the complete picture
- Focus on user benefits rather than technical implementation
- Explain why this feature matters to users in real-world usage

Write directly from the user's perspective about what they can accomplish and the complete value they receive from this feature.
"""

    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "You help users understand software features by writing comprehensive, user-focused descriptions. Always start responses with 'I want to' and explain what users can accomplish and the practical value they receive. Synthesize information from multiple sources to show complete user benefits."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=1000
        )
        
        content = response.choices[0].message.content
        return content if content is not None else feature_description
        
    except Exception as e:
        print(f"⚠️ LLM failed to generate feature detailed description: {e}")
        return None

# --- Main Function ---

def analyze_pr(repo_name: str, pr_url: str, feature_description: str, use_cache: bool = True) -> Optional[PRAnalysis]:
    """Analyze a single PR"""
    pr_number = extract_pr_number_from_url(pr_url)
    if not pr_number:
        print(f"⚠️ Unable to extract PR number from URL: {pr_url}")
        return None
    
    cache_key = f"{repo_name}#{pr_number}"
    
    # Check cache
    if use_cache:
        cache = load_pr_analysis_cache()
        if cache_key in cache:
            print(f"  > 🔄 Loading PR#{pr_number} analysis result from cache")
            return cache[cache_key]
    
    print(f"  > 🔍 Analyzing PR#{pr_number}...")
    
    # Get PR basic info
    pr_info = get_pr_info(repo_name, pr_number)
    if not pr_info:
        return None
    
    # Get file changes
    file_changes = get_pr_files(repo_name, pr_number)
    
    # Get detailed commit info
    base_sha = pr_info['base']['sha']
    head_sha = pr_info['head']['sha']
    
    base_commit = get_commit_info(repo_name, base_sha)
    head_commit = get_commit_info(repo_name, head_sha)
    
    # If unable to get commit info, create basic Commit objects
    if not base_commit:
        base_commit = Commit(sha=base_sha, message='', date='', author='')
    if not head_commit:
        head_commit = Commit(sha=head_sha, message='', date='', author='')
    
    # Get detailed commit info
    base_commit = get_commit_info(repo_name, base_sha)
    head_commit = get_commit_info(repo_name, head_sha)
    
    # If unable to get commit info, create basic Commit objects
    if not base_commit:
        base_commit = Commit(sha=base_commit.sha, message='', date='', author='')
    if not head_commit:
        head_commit = Commit(sha=head_commit.sha, message='', date='', author='')

    test_files = []
    non_test_files = []
    detailed_description = None
    only_modified_existing_functions = True

    for file_data in file_changes:
        file_path = file_data.filename
        
        if is_test_file(file_path):
            test_files.append(file_path)
            print(f"    - Found test file: {file_path}")
        elif file_path.endswith('.py'):  # Only analyze Python files
            non_test_files.append(file_path)
            
            # Check function changes
            status = file_data.status
            
            # For added or removed files, as long as they contain function definitions, condition not met
            if status == 'added' or status == 'removed':
                content = get_file_content(repo_name, file_path, head_commit.sha if status == 'added' else base_commit.sha)
                if content and extract_definitions(content):
                    print(f"    - ⚠️ Found {status} file containing function definitions: {file_path}")
                    only_modified_existing_functions = False
                    break
            
            # For modified files, need to compare function definitions before and after modification
            elif status == 'modified':
                before_content = get_file_content(repo_name, file_path, base_commit.sha)
                after_content = get_file_content(repo_name, file_path, head_commit.sha)

                # Handle case where file content might be None
                if before_content is not None and after_content is not None:
                    only_modified, new_funcs, deleted_funcs = analyze_function_changes(before_content, after_content)
                
                    if not only_modified:
                        print(f"    - ⚠️ File modification contains function additions or deletions: {file_path}")
                        if new_funcs:
                            print(f"      Added functions: {', '.join(new_funcs)}")
                        if deleted_funcs:
                            print(f"      Deleted functions: {', '.join(deleted_funcs)}")
                        only_modified_existing_functions = False
                        break
                else:
                    print(f"    - ⚠️ Unable to get file content, may be empty or non-existent: {file_path}")
                    only_modified_existing_functions = False
    
    # Generate detailed description
    if only_modified_existing_functions and test_files and non_test_files:
        detailed_description = generate_detailed_description_with_llm(
            feature_description, pr_info, file_changes
        )
    
    if not detailed_description:
        if not test_files:
            print(f"  > ⏭️ PR#{pr_number} does not contain test file changes, skipping analysis")
        elif not non_test_files:
            print(f"  > ⏭️ PR#{pr_number} does not contain non-test file changes, skipping analysis")
        elif not only_modified_existing_functions:
            print(f"  > ⏭️ PR#{pr_number} contains function additions or deletions, skipping analysis")
        return None
    
    analysis = PRAnalysis(
        pr_number=pr_number,
        repo_name=repo_name,
        title=pr_info.get('title', ''),
        description=pr_info.get('body', ''),
        state=pr_info.get('state', ''),
        merged=pr_info.get('merged', False),
        base_commit=base_commit,
        head_commit=head_commit,
        file_changes=file_changes,
        detailed_description=detailed_description,
        has_tests=len(test_files) > 0,
        test_files=test_files,
        only_modified_existing_functions=only_modified_existing_functions,
        non_test_files=non_test_files,
        analyzed_at=time.strftime('%Y-%m-%d %H:%M:%S')
    )

    print(f"    - PR#{pr_number}: has_tests={analysis.has_tests}, test_file_count={len(analysis.test_files)}, only_modified_functions={analysis.only_modified_existing_functions}, only_modified_existing={analysis.only_modified_existing_functions}")

    # Save to cache
    if use_cache:
        save_pr_analysis_to_cache(analysis)
    
    return analysis

def enhance_feature_with_pr_analysis(feature, repo_name: str) -> Optional[EnhancedFeature]:
    """Enhance feature object by adding PR detailed analysis"""
    pr_analyses = []
    
    # Use tqdm to show PR analysis progress
    with tqdm(feature.pr_links, desc=f"Analyzing PRs", unit="pr", leave=False) as pbar:
        for pr_link in pbar:
            pr_number = extract_pr_number_from_url(pr_link)
            pbar.set_description(f"PR#{pr_number}")
            
            pr_analysis = analyze_pr(repo_name, pr_link, feature.description)
            if pr_analysis:
                pr_analyses.append(pr_analysis)
            
            # Avoid API rate limit
            time.sleep(0.5)
    
    # If only one PR, use PR's detailed description directly
    if len(pr_analyses) == 1:
        feature_detailed_description = pr_analyses[0].detailed_description
    elif len(pr_analyses) > 1:
        # For multiple PRs, generate feature detailed description based on all PR analyses
        feature_detailed_description = generate_feature_detailed_description(
            feature.description,
            feature.feature_type,
            pr_analyses
        )
    else:
        return None
    
    if feature_detailed_description:
        return EnhancedFeature(
            feature_type=feature.feature_type,
            description=feature.description,
            pr_analyses=pr_analyses,
            feature_detailed_description=feature_detailed_description
        )
    else:
        return None

def enhance_release_analysis_with_pr_details(release_analysis) -> List[EnhancedFeature]:
    """Enhance release analysis, only process new_features to add PR details"""
    print(f"--- Starting analysis of new features for {release_analysis.tag_name} ---")
    
    enhanced_features = []
    
    # Use tqdm to show feature analysis progress
    with tqdm(release_analysis.new_features, desc=f"Analyzing features", unit="feature", leave=False) as pbar:
        for feature in pbar:
            if feature.pr_links:
                pbar.set_description(f"Feature: {feature.description[:30]}...")
                enhanced = enhance_feature_with_pr_analysis(feature, release_analysis.repo_name)
                if enhanced:
                    enhanced_features.append(enhanced)
                    pbar.write(f"    ✅ Analyzed feature: {feature.description[:50]}...")
                else:
                    pbar.write(f"    ⚠️ Skipped feature: {feature.description[:50]}...")
    
    return enhanced_features